{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1a8fff-9e77-4114-9700-3ae725a24a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d88e30b-9d2d-4254-906f-f19350826402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq_utils import read_fasta, read_fastq_barcode\n",
    "# Read the FASTA file containing all human cDNA sequences.\n",
    "cdnas = read_fasta('./data/Homo_sapiens.GRCh38.cdna.all.fa')\n",
    "# Read the FASTQ file containing an 10x single cell sequencing experiment\n",
    "seq_path = './data/500_PBMC_3p_LT_Chromium_X_S4_L004_R2_001.fastq_ds100000'\n",
    "bc_path = './data/500_PBMC_3p_LT_Chromium_X_S4_L004_R1_001.fastq_ds100000'\n",
    "sequences = read_fastq_barcode(seq_path, bc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb4a62b-f793-4488-95d8-d76980d86ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduce size of the cdnas to make benchmarking more concistant\n",
    "## since the sequencing is done on the 3' end, the tail of the cDNA is kept\n",
    "for i in cdnas:\n",
    "    if len(i['sequence']) > 1000:\n",
    "        i['sequence'] = i['sequence'][-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878a0919-c7e8-46b5-b726-d7097075496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "# Set the HDFS URL. 'host.docker.internal' allows the Docker container to communicate with the host machine.\n",
    "# The port 9870 is typically used for the HDFS web UI.\n",
    "hdfs_url        = 'http://host.docker.internal:9870'\n",
    "ubuntu_Benutzer = 'alfa'\n",
    "\n",
    "# Create an instance of the InsecureClient class to interact with HDFS.\n",
    "client = InsecureClient(hdfs_url, user=ubuntu_Benutzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1decd308-5795-4c3a-9c51-7d9d1031d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "filepath_cdna = '/bigdata/cdna_align'\n",
    "filepath_seq = '/bigdata/seq_align'\n",
    "from benchmark_utils import writeToHadoop, deleteTestSet, align_pyspark, align_hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ec90a1-d26f-481f-a5e1-f33fb8a39a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "start_time = f'{time():.0f}' # Get current time in seconds for logging file name\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename='./benchmark/align_benchmark_' + start_time + '.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "runtimes = []\n",
    "nrun = 0\n",
    "# for len_cdna in [5, 1]:\n",
    "#     for len_seq in [5, 1]:\n",
    "for len_cdna in [100, 80, 60, 40, 20, 10, 5, 1]: # Loop over different sequence array lengths\n",
    "    for len_seq in [100, 80, 60, 40, 20, 10, 5, 1]: # Loop over different cdna array lengths\n",
    "\n",
    "        # Sample a subset of cDNAs from the loaded dataset\n",
    "        cdnas_ds = sample(cdnas, len_cdna)\n",
    "        writeToHadoop(client, filepath_cdna, cdnas_ds)\n",
    "        # Sample a subset of sequences from the loaded dataset\n",
    "        sequences_ds = sample(sequences, len_seq)\n",
    "        writeToHadoop(client, filepath_seq, sequences_ds)\n",
    "            \n",
    "        # Log information about the current run\n",
    "        logging.info(f\"Run number {nrun+1:.0f} with {len_cdna:.0f} cdnas and {len_seq:.0f} as sequences\")\n",
    "\n",
    "        # Execute sequence alignment using PySpark and measure runtime\n",
    "        spark_time, spark_result = align_pyspark(filepath_cdna, filepath_seq)\n",
    "        logging.info(f\"Spark alignment runtime: {spark_time:.4f} seconds\")\n",
    "        logging.info(\"Spark alignment results: \" + str(spark_result))\n",
    "        runtimes.append(('Spark', nrun, len_cdna, len_seq, spark_time))\n",
    "            \n",
    "        # Execute sequence alignment using Hadoop and measure runtime\n",
    "        hadoop_time, hadoop_result = align_hadoop(filepath_cdna, filepath_seq)\n",
    "        logging.info(f\"Hadoop alignment runtime: {hadoop_time:.4f} seconds\")\n",
    "        logging.info(\"Hadoop alignment results: \" + str(hadoop_result))\n",
    "        runtimes.append(('Hadoop', nrun, len_cdna, len_seq, hadoop_time))\n",
    "\n",
    "        # Compare results from Spark and Hadoop BLASTN\n",
    "        if sorted(spark_result) == sorted(hadoop_result):\n",
    "            logging.info(\"Spark and Hadoop alignment results are matching\")\n",
    "        else:\n",
    "            logging.warning(\"Spark and Hadoop alignment results does not match\")\n",
    "\n",
    "        # Delete the test set from Hadoop HDFS using the client and specified filepath\n",
    "        deleteTestSet(client, filepath_cdna)\n",
    "        deleteTestSet(client, filepath_seq)\n",
    "\n",
    "# Write the collected runtimes to a text file\n",
    "with open('./benchmark/align_runtimes_' + start_time + '.txt', 'w') as f:\n",
    "    for line in runtimes:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "186062ee-d870-477a-9380-0a16e8dca18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20.25559091567993, [('ENST00000507135.5', 1), ('ENST00000611279.4', 1), ('ENST00000640046.1', 2), ('ENST00000559515.1', 1)])\n",
      "(111.48353052139282, [])\n",
      "func took: 131.8595 sec\n"
     ]
    }
   ],
   "source": [
    "len_cdna = 5\n",
    "len_seq = 5\n",
    "\n",
    "ts = time()\n",
    "\n",
    "cdnas_ds = sample(cdnas, len_cdna)\n",
    "writeToHadoop(client, filepath_cdna, cdnas_ds)\n",
    "sequences_ds = sample(sequences, len_seq)\n",
    "writeToHadoop(client, filepath_seq, sequences_ds)\n",
    "\n",
    "temp_p = align_pyspark(filepath_cdna, filepath_seq)\n",
    "temp_h = align_hadoop(filepath_cdna, filepath_seq)\n",
    "\n",
    "deleteTestSet(client, filepath_cdna)\n",
    "deleteTestSet(client, filepath_seq)\n",
    "\n",
    "te = time()\n",
    "\n",
    "print(temp_p)\n",
    "print(temp_h)\n",
    "print('func took: %2.4f sec' % (te-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd9b575-15cc-4c92-be20-3a8fa0a44ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(temp_p[1]) == sorted(temp_h[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0f64cb-1ec6-45b6-a034-51100c07bc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('TATGGGCCTAGGTCTTCGGTGTTTACACATTAATGGCAAGAAAACGGTAGGGAGAGCGTG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709d559-09e7-45af-9773-46547cf15385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

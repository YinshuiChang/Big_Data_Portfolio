{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "832dfcf7-fb7a-4aee-b0f4-fec0c1439259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37de70f-55c1-4ae5-a0cf-80ff586093ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq_utils import read_fasta\n",
    "# Read the FASTA file containing all human cDNA sequences.\n",
    "cdnas = read_fasta('./data/Homo_sapiens.GRCh38.cdna.all.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6fd8171-d29a-492d-9cca-b48d5144b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduce size of the cdnas to make benchmarking more concistant\n",
    "## since the sequencing is done on the 3' end, the tail of the cDNA is kept\n",
    "for i in cdnas:\n",
    "    if len(i['sequence']) > 1000:\n",
    "        i['sequence'] = i['sequence'][-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b2c8f5-375a-440f-88cd-a5ac65f1d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "# Set the HDFS URL. 'host.docker.internal' allows the Docker container to communicate with the host machine.\n",
    "# The port 9870 is typically used for the HDFS web UI.\n",
    "hdfs_url        = 'http://host.docker.internal:9870'\n",
    "ubuntu_Benutzer = 'alfa'\n",
    "\n",
    "# Create an instance of the InsecureClient class to interact with HDFS.\n",
    "client = InsecureClient(hdfs_url, user=ubuntu_Benutzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06d3ad8-880d-41a7-a9c7-813e8bdc2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from benchmark_utils import generate_random_dna_sequence, writeToHadoop, deleteTestSet\n",
    "filepath = '/bigdata/cdna_blastn'\n",
    "from benchmark_utils import blastn_pyspark, blastn_hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8864e539-2223-41aa-8d95-218b2e6f4e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "start_time = f'{time():.0f}' # Get current time in seconds for logging file name\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename='./benchmark/blastn_benchmark_' + start_time + '.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "runtimes = []\n",
    "# for nrun in range(3):\n",
    "#     for len_arr in [20000]:\n",
    "#         for len_query in [80]:\n",
    "nrun = 0\n",
    "for len_query in [80, 60, 40, 20, 10]: # Loop over different query sequence lengths\n",
    "    for len_arr in [20000, 15000, 10000, 5000, 2000, 1000, 100, 10, 1]: # Loop over different cdna array lengths\n",
    "        # Generate random DNA sequence with specified length and weights for nucleotides\n",
    "        query_sequence = generate_random_dna_sequence(len_query, weights= {'A': 6, 'C': 4, 'G': 4, 'T': 6})\n",
    "        # Sample a subset of cDNAs from the loaded dataset\n",
    "        cdnas_ds = sample(cdnas, len_arr)\n",
    "        # Write the sampled cDNAs to Hadoop HDFS using the client and specified filepath\n",
    "        writeToHadoop(client, filepath, cdnas_ds)\n",
    "            \n",
    "        # Log information about the current run\n",
    "        logging.info(f\"Run number {nrun+1:.0f} with {len_arr:.0f} sequences and {query_sequence} as query sequence\")\n",
    "\n",
    "        # Execute BLASTN using PySpark and measure runtime\n",
    "        spark_time, spark_result = blastn_pyspark(query_sequence, filepath)\n",
    "        logging.info(f\"Spark BLASTN runtime: {spark_time:.4f} seconds\")\n",
    "        logging.info(\"Spark BLASTN results: \" + str(spark_result))\n",
    "        runtimes.append(('Spark', nrun, len_arr, len_query, spark_time))\n",
    "            \n",
    "        # Execute BLASTN using Hadoop and measure runtime\n",
    "        hadoop_time, hadoop_result = blastn_hadoop(query_sequence, filepath)\n",
    "        logging.info(f\"Hadoop BLASTN runtime: {hadoop_time:.4f} seconds\")\n",
    "        logging.info(\"Hadoop BLASTN results: \" + str(hadoop_result))\n",
    "        runtimes.append(('Hadoop', nrun, len_arr, len_query, hadoop_time))\n",
    "\n",
    "        # Compare results from Spark and Hadoop BLASTN\n",
    "        if sorted(spark_result) == sorted(hadoop_result):\n",
    "            logging.info(\"Spark and Hadoop BLASTN results are matching\")\n",
    "        else:\n",
    "            logging.warning(\"Spark and Hadoop BLASTN results does NOT match\")\n",
    "\n",
    "        # Delete the test set from Hadoop HDFS using the client and specified filepath\n",
    "        deleteTestSet(client, filepath)\n",
    "\n",
    "# Write the collected runtimes to a text file\n",
    "with open('./benchmark/blastn_runtimes_' + start_time + '.txt', 'w') as f:\n",
    "    for line in runtimes:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b28deae5-2fce-414d-a45c-d910238f8998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 0, 20000, 80, 2556.827931880951),\n",
       " ('Hadoop', 0, 20000, 80, 1388.7604849338531),\n",
       " ('Spark', 0, 15000, 80, 1707.871378660202),\n",
       " ('Hadoop', 0, 15000, 80, 980.1165933609009),\n",
       " ('Spark', 0, 10000, 80, 1170.049753189087),\n",
       " ('Hadoop', 0, 10000, 80, 661.6926681995392),\n",
       " ('Spark', 0, 5000, 80, 548.5153164863586),\n",
       " ('Hadoop', 0, 5000, 80, 345.3649334907532),\n",
       " ('Spark', 0, 2000, 80, 221.74194884300232),\n",
       " ('Hadoop', 0, 2000, 80, 166.25478982925415),\n",
       " ('Spark', 0, 1000, 80, 116.97669506072998),\n",
       " ('Hadoop', 0, 1000, 80, 100.02249145507812),\n",
       " ('Spark', 0, 100, 80, 13.289900779724121),\n",
       " ('Hadoop', 0, 100, 80, 43.869657039642334),\n",
       " ('Spark', 0, 10, 80, 3.1744937896728516),\n",
       " ('Hadoop', 0, 10, 80, 37.24662685394287),\n",
       " ('Spark', 0, 1, 80, 1.965494155883789),\n",
       " ('Hadoop', 0, 1, 80, 36.30191206932068),\n",
       " ('Spark', 0, 20000, 60, 1639.8343772888184),\n",
       " ('Hadoop', 0, 20000, 60, 976.2617943286896),\n",
       " ('Spark', 0, 15000, 60, 1269.9237580299377),\n",
       " ('Hadoop', 0, 15000, 60, 756.3479599952698),\n",
       " ('Spark', 0, 10000, 60, 840.8849139213562),\n",
       " ('Hadoop', 0, 10000, 60, 518.2708730697632),\n",
       " ('Spark', 0, 5000, 60, 423.59321546554565),\n",
       " ('Hadoop', 0, 5000, 60, 279.7089157104492),\n",
       " ('Spark', 0, 2000, 60, 170.98100423812866),\n",
       " ('Hadoop', 0, 2000, 60, 133.4510998725891),\n",
       " ('Spark', 0, 1000, 60, 88.68037867546082),\n",
       " ('Hadoop', 0, 1000, 60, 85.32529139518738),\n",
       " ('Spark', 0, 100, 60, 10.415045261383057),\n",
       " ('Hadoop', 0, 100, 60, 42.472803354263306),\n",
       " ('Spark', 0, 10, 60, 2.771885633468628),\n",
       " ('Hadoop', 0, 10, 60, 37.4350221157074),\n",
       " ('Spark', 0, 1, 60, 1.8674259185791016),\n",
       " ('Hadoop', 0, 1, 60, 36.53309917449951),\n",
       " ('Spark', 0, 20000, 40, 1132.4055120944977),\n",
       " ('Hadoop', 0, 20000, 40, 695.7948131561279),\n",
       " ('Spark', 0, 15000, 40, 858.1034142971039),\n",
       " ('Hadoop', 0, 15000, 40, 529.7846884727478),\n",
       " ('Spark', 0, 10000, 40, 567.9205532073975),\n",
       " ('Hadoop', 0, 10000, 40, 356.0169758796692),\n",
       " ('Spark', 0, 5000, 40, 287.7759213447571),\n",
       " ('Hadoop', 0, 5000, 40, 198.14209270477295),\n",
       " ('Spark', 0, 2000, 40, 117.89951610565186),\n",
       " ('Hadoop', 0, 2000, 40, 102.68625926971436),\n",
       " ('Spark', 0, 1000, 40, 62.92966294288635),\n",
       " ('Hadoop', 0, 1000, 40, 68.80279231071472),\n",
       " ('Spark', 0, 100, 40, 7.9570300579071045),\n",
       " ('Hadoop', 0, 100, 40, 40.482346057891846),\n",
       " ('Spark', 0, 10, 40, 2.427568197250366),\n",
       " ('Hadoop', 0, 10, 40, 37.235434770584106),\n",
       " ('Spark', 0, 1, 40, 1.8188998699188232),\n",
       " ('Hadoop', 0, 1, 40, 36.874579191207886),\n",
       " ('Spark', 0, 20000, 20, 572.3886196613312),\n",
       " ('Hadoop', 0, 20000, 20, 365.4707062244415),\n",
       " ('Spark', 0, 15000, 20, 435.95007157325745),\n",
       " ('Hadoop', 0, 15000, 20, 295.23256754875183),\n",
       " ('Spark', 0, 10000, 20, 297.9278812408447),\n",
       " ('Hadoop', 0, 10000, 20, 205.29972290992737),\n",
       " ('Spark', 0, 5000, 20, 149.91543793678284),\n",
       " ('Hadoop', 0, 5000, 20, 120.78295135498047),\n",
       " ('Spark', 0, 2000, 20, 61.429975748062134),\n",
       " ('Hadoop', 0, 2000, 20, 71.1331627368927),\n",
       " ('Spark', 0, 1000, 20, 31.157438039779663),\n",
       " ('Hadoop', 0, 1000, 20, 54.7689003944397),\n",
       " ('Spark', 0, 100, 20, 4.704575300216675),\n",
       " ('Hadoop', 0, 100, 20, 39.17751407623291),\n",
       " ('Spark', 0, 10, 20, 2.107254981994629),\n",
       " ('Hadoop', 0, 10, 20, 37.89290738105774),\n",
       " ('Spark', 0, 1, 20, 1.8148887157440186),\n",
       " ('Hadoop', 0, 1, 20, 37.067700147628784),\n",
       " ('Spark', 0, 20000, 10, 349.31034231185913),\n",
       " ('Hadoop', 0, 20000, 10, 269.5635771751404),\n",
       " ('Spark', 0, 15000, 10, 306.6056365966797),\n",
       " ('Hadoop', 0, 15000, 10, 225.49214339256287),\n",
       " ('Spark', 0, 10000, 10, 192.14618229866028),\n",
       " ('Hadoop', 0, 10000, 10, 162.69912195205688),\n",
       " ('Spark', 0, 5000, 10, 99.94092559814453),\n",
       " ('Hadoop', 0, 5000, 10, 106.22259378433228),\n",
       " ('Spark', 0, 2000, 10, 44.29985070228577),\n",
       " ('Hadoop', 0, 2000, 10, 77.40928077697754),\n",
       " ('Spark', 0, 1000, 10, 23.439211130142212),\n",
       " ('Hadoop', 0, 1000, 10, 66.01175475120544),\n",
       " ('Spark', 0, 100, 10, 4.54027247428894),\n",
       " ('Hadoop', 0, 100, 10, 55.186819553375244),\n",
       " ('Spark', 0, 10, 10, 2.4993932247161865),\n",
       " ('Hadoop', 0, 10, 10, 53.59824872016907),\n",
       " ('Spark', 0, 1, 10, 2.0504281520843506),\n",
       " ('Hadoop', 0, 1, 10, 53.894198179244995)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e79475d2-5957-4c56-b4d3-518768f92155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.715787410736084, [('ENST00000476472.5', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000410037.5', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000427765.1', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000531951.6', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000552917.1', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000550912.1', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000676748.1', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000633875.4', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000360096.3', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000556353.1', 10.0, 'CTGCC', 'CTGCC')])\n",
      "(63.40727353096008, [('ENST00000552917.1', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000531951.6', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000427765.1', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000410037.5', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000476472.5', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000556353.1', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000360096.3', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000633875.4', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000676748.1', 10.0, 'CTGCC', 'CTGCC'), ('ENST00000550912.1', 10.0, 'CTGCC', 'CTGCC')])\n"
     ]
    }
   ],
   "source": [
    "len_query = 5\n",
    "len_arr = 10\n",
    "\n",
    "query_sequence = generate_random_dna_sequence(len_query, weights= {'A': 6, 'C': 4, 'G': 4, 'T': 6})\n",
    "cdnas_ds = sample(cdnas, len_arr)\n",
    "writeToHadoop(client, filepath, cdnas_ds)\n",
    "\n",
    "temp_p = blastn_pyspark(query_sequence, filepath)\n",
    "temp_h = blastn_hadoop(query_sequence, filepath)\n",
    "\n",
    "deleteTestSet(client, filepath)\n",
    "\n",
    "print(temp_p)\n",
    "print(temp_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15bb4ccd-02f3-4deb-b888-506dbf0f97e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(temp_p[1]) == sorted(temp_h[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394c93b-af5a-48a4-8939-b7473ea8a60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a464e535-a578-4769-9946-27befb29d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            deleteTestSet(client, filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
